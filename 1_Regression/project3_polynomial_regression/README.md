# Project 3 â€” Polynomial Regression (sklearn)

A hands-on implementation of Polynomial Regression using scikit-learn. Polynomial feature expansion lets a simple linear model learn curved relationships while remaining easy to train, interpret, and compare against the earlier linear projects.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Requirements](#requirements)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Concepts Covered](#concepts-covered)
- [Degree Selection & Model Behavior](#degree-selection--model-behavior)
- [Evaluation Metrics](#evaluation-metrics)
- [Example Results](#example-results)
- [Visualizations](#visualizations)
- [Why Project 3 Matters](#why-project-3-matters)
- [Next Steps](#next-steps)

## ğŸ¯ Overview

This project demonstrates how polynomial feature expansion (`x, xÂ², xÂ³, â€¦, xáµˆ`) enables linear regression to fit non-linear relationships. It compares different polynomial degrees to illustrate underfitting, good fit, and overfitting, and highlights how scikit-learn handles feature expansion and closed-form regression efficiently.

## âœ¨ Features

- **Polynomial Feature Expansion**: Uses `PolynomialFeatures` to create non-linear terms.
- **Multiple Degrees Compared**: Trains degrees 1, 2, 3, and 10 to show under/overfitting.
- **Fast Training**: Relies on scikit-learnâ€™s analytical OLS solution.
- **Comprehensive Evaluation**: Train/Test MSE and RÂ² across degrees.
- **Visualization**: Plots fitted curves for each degree alongside data.
- **Reproducible Data**: Synthetic cubic dataset with fixed random seed.

## ğŸ“¦ Requirements

- Python 3.7+
- NumPy
- scikit-learn
- Matplotlib (for plots)

## ğŸ“‚ Project Structure

```
project3_polynomial_regression/
â”‚
â”œâ”€â”€ data.py        # Synthetic cubic dataset
â”œâ”€â”€ train.py       # Trains models for degrees 1, 2, 3, 10
â”œâ”€â”€ eval.py        # Computes Train/Test MSE and RÂ²
â”œâ”€â”€ plots.py       # Generates fitted-curve plots
â”œâ”€â”€ plots.png      # Saved plot (generated by plots.py)
â””â”€â”€ README.md      # Project documentation
```

## ğŸš€ Usage

### 1) Train all models

```bash
python train.py
```

Outputs coefficients and intercepts for degrees 1, 2, 3, 10.

### 2) Evaluate model performance

```bash
python eval.py
```

Outputs Train/Test MSE and RÂ² for each degree.

### 3) Visualize fitted curves

```bash
python plots.py
```

Displays and can save `plots.png` with curves for all degrees.

## ğŸ“š Concepts Covered

### Polynomial Feature Expansion
- Transforms `X` into `[x, xÂ², xÂ³, â€¦]` via `PolynomialFeatures(degree, include_bias=False)`.
- `include_bias=False` avoids duplicating the intercept term already handled by `LinearRegression`.

### Synthetic Data
- Function: `y = 0.5xÂ³ - 2xÂ² + 3x + 5 + noise`.
- Generation: `np.linspace(-5, 5, 100)` with Gaussian noise; `np.random.seed(42)` for reproducibility.
- Purpose: Controlled environment to observe under/overfitting on a known cubic relationship.

### Underfitting vs Overfitting
- Low degree (1): Too simple, misses curvature.
- Moderate degree (3): Matches true pattern well.
- High degree (10): Wiggly curve that memorizes noise.

### Biasâ€“Variance Intuition
- Increasing degree lowers bias but raises variance.
- Model selection uses validation/test metrics, not just training error.

## ğŸ” Degree Selection & Model Behavior

| Degree | Shape       | Behavior   |
|--------|-------------|------------|
| 1      | Line        | âŒ Underfit |
| 2      | Parabola    | ğŸ‘ Good     |
| 3      | Cubic       | â­ Best Fit |
| 10     | Highly wavy | âš ï¸ Overfit  |

Practical takeaway: choose the degree that minimizes test error and generalizes (here, degree 3).

## ğŸ“Š Evaluation Metrics

- **Train MSE / Test MSE**: Error on train vs. held-out data.
- **RÂ² Score**: Variance explained; closer to 1 is better.
- **Inference Time (optional)**: Polynomial terms add cost, but small degrees remain fast.

## ğŸ§ª Example Results

From the provided synthetic run:

| Degree | Train MSE | Test MSE | RÂ² Score | Interpretation |
|--------|-----------|----------|----------|----------------|
| 1      | 326.64    | 455.62   | 0.712    | âŒ Underfit    |
| 2      | 137.29    | 143.74   | 0.909    | ğŸ‘ Good        |
| 3      | 81.60     | 60.22    | 0.962    | â­ Best        |
| 10     | 74.93     | 81.73    | 0.948    | âš ï¸ Overfit     |

Selected model: **Degree 3** (best generalization, matches true cubic pattern).

## ğŸ–¼ï¸ Visualizations

`plots.py` plots data points plus fitted curves for degrees 1, 2, 3, 10:
- Degree 1 â†’ underfits (straight line)
- Degree 3 â†’ natural, smooth S-shape (best)
- Degree 10 â†’ wiggly, captures noise (overfits)

Save or view the generated figure as `plots.png`.

## ğŸ“ Why Project 3 Matters

- Connects linear models to non-linear patterns via feature engineering.
- Illustrates biasâ€“variance trade-off using degree as complexity control.
- Validates scikit-learn workflows after scratch and basic sklearn linear regression.
- Prepares for regularization (controlling overfitting) and more complex models.

## ğŸŒŸ Next Steps

**Project 4 â€” Ridge & Lasso Regression**
- Apply L2 (Ridge) and L1 (Lasso) regularization to stabilize high-degree models.
- Control large coefficients and reduce overfitting (especially for degree 10).
- Learn how to tune regularization strength (`alpha`) and compare outcomes.

---

**Note**: This project is part of the Machine Learning Specialization series, building on Projects 1 and 2 to extend linear models to non-linear relationships while keeping training efficient.


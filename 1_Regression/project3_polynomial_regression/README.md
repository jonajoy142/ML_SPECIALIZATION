# Project 3 â€” Polynomial Regression (sklearn)

A comprehensive implementation of Polynomial Regression using scikit-learn. Polynomial feature expansion enables linear models to capture curved, non-linear relationships while maintaining the simplicity and efficiency of linear regression.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Requirements](#requirements)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Concepts Covered](#concepts-covered)
- [Why Polynomial Regression?](#why-polynomial-regression)
- [Synthetic Data Generation](#synthetic-data-generation)
- [Polynomial Feature Expansion](#polynomial-feature-expansion)
- [Degree Selection & Model Behavior](#degree-selection--model-behavior)
- [Evaluation Metrics & Results](#evaluation-metrics--results)
- [Real-World Example: Hours Studied](#real-world-example-hours-studied)
- [Real-World Applications](#real-world-applications)
- [Visualizations](#visualizations)
- [Final Takeaways](#final-takeaways)
- [Next Steps](#next-steps)

## ğŸ¯ Overview

Polynomial Regression allows us to model curved relationships by expanding input features into polynomial terms:

```
x, xÂ², xÂ³, ..., xáµˆ
```

Using these expanded features, a simple `LinearRegression` model can learn non-linear curves while still being easy to train and understand. This project demonstrates how different polynomial degrees affect model performance, illustrating concepts of underfitting, optimal fit, and overfitting.

## âœ¨ Features

- **Polynomial Feature Expansion**: Uses `PolynomialFeatures` to transform features into polynomial terms
- **Multiple Degrees Compared**: Trains models with degrees 1, 2, 3, and 10 to demonstrate underfitting and overfitting
- **Fast Training**: Leverages scikit-learn's analytical OLS solution
- **Comprehensive Evaluation**: Computes Train/Test MSE and RÂ² scores for model comparison
- **Visualization**: Generates plots showing fitted curves for all degrees
- **Reproducible Data**: Synthetic cubic dataset with fixed random seed for consistent results

## ğŸ“¦ Requirements

- Python 3.7+
- NumPy
- scikit-learn
- Matplotlib (for visualization)

## ğŸ“‚ Project Structure

```
project3_polynomial_regression/
â”‚
â”œâ”€â”€ data.py        # Synthetic cubic dataset generation
â”œâ”€â”€ train.py       # Trains models for degrees 1, 2, 3, 10
â”œâ”€â”€ eval.py        # Computes MSE, RÂ² on train/test sets
â”œâ”€â”€ plots.py       # Generates fitted-curve visualizations
â”œâ”€â”€ plots.png      # Saved visualization (generated by plots.py)
â””â”€â”€ README.md      # Project documentation
```

## ğŸš€ Usage

### 1ï¸âƒ£ Train All Models

```bash
python train.py
```

This script trains polynomial regression models for degrees 1, 2, 3, and 10, displaying coefficients and intercepts for each.

### 2ï¸âƒ£ Evaluate Model Performance

```bash
python eval.py
```

This script computes and displays:
- Train MSE and Test MSE for each degree
- RÂ² Score for model comparison
- Performance metrics to identify best model

### 3ï¸âƒ£ Visualize Fitted Curves

```bash
python plots.py
```

This script generates and displays plots showing:
- Original data points
- Fitted curves for each polynomial degree
- Visual comparison of underfitting vs. overfitting

## ğŸ“š Concepts Covered

### What You Learn in This Project

- âœ” **Polynomial Feature Expansion**: How `PolynomialFeatures` transforms `X â†’ [x, xÂ², xÂ³, â€¦]`
- âœ” **Meaning of Polynomial Degrees**: Understanding how degree affects model flexibility
- âœ” **Synthetic Data Generation**: Why synthetic data is useful for learning and experimentation
- âœ” **Training Multiple Models**: Comparing different polynomial degrees (1, 2, 3, 10)
- âœ” **Understanding Metrics**: Train MSE, Test MSE, RÂ² Score, and their interpretation
- âœ” **Underfitting vs. Overfitting**: Recognizing model complexity issues visually and numerically
- âœ” **Plotting & Visualization**: How to view model behavior and detect overfitting

## ğŸ” Why Polynomial Regression?

### The Problem with Linear Regression

Regular Linear Regression fits:

```
y = Î¸x + b   # straight line
```

But real-world problems are often **curved, not straight**.

### Real-World Examples

| Scenario | Relationship | Why Non-Linear? |
|----------|--------------|-----------------|
| **Hours studied â†’ Exam marks** | Curved | More study increases marks, but later plateaus (diminishing returns) |
| **Speed â†’ Fuel efficiency** | U-shaped curve | Too slow or too fast = bad mileage |
| **Temperature â†’ Electricity usage** | Curved | Very hot or very cold â†’ high AC/heater usage |

**Polynomial regression captures such curves** by expanding features into polynomial terms, allowing linear models to fit non-linear relationships.

## ğŸ§ª Synthetic Data Generation

### Data Generation Formula

```python
X = np.linspace(-5, 5, 100)
y = 0.5*X**3 - 2*X**2 + 3*X + 5 + noise
```

### Why Synthetic Data?

Using synthetic data is perfect for learning because:

- âœ… **We control the true mathematical function** - We know the exact relationship
- âœ… **We know the relationship is cubic** - Makes it easy to validate model performance
- âœ… **We can simulate real-world noise** - Adds realism while maintaining control
- âœ… **It helps observe underfitting and overfitting clearly** - Controlled environment for learning

### Why `np.linspace(-5, 5, 100)`?

Creates 100 evenly spaced values between -5 and 5. This symmetric range is ideal because:

- Cubic curves change direction
- Helps visualize S-shape behavior
- Provides good coverage of the function's behavior

### Why `np.random.seed(42)`?

Guarantees same noise every run, ensuring:

- âœ… Training results stay consistent
- âœ… Easier debugging
- âœ… Reproducible learning experience

## ğŸ”§ Polynomial Feature Expansion

### Core Concept

**Example Transformation:**

If:
```
X = [2]
```

Degree = 3 creates:
```
[2, 4, 8]  # [x, xÂ², xÂ³]
```

### Implementation in sklearn

```python
poly = PolynomialFeatures(degree=degree, include_bias=False)
X_poly = poly.fit_transform(X)
```

### Why `include_bias=False`?

Because `LinearRegression()` already adds its own bias term. Otherwise, we would add a duplicate column of 1s, which is redundant.

## ğŸ“Š Degree Selection & Model Behavior

### Why We Train 4 Models (degree = 1, 2, 3, 10)

Each degree gives a different level of flexibility:

| Degree | Shape | Behavior |
|--------|-------|----------|
| 1 | Straight line | âŒ Too simple (underfit) |
| 2 | Parabola | ğŸ‘ Fairly good |
| 3 | Cubic curve | â­ Best fit |
| 10 | Highly wiggly curve | âš ï¸ Overfits |

This project teaches **model selection** - choosing the right complexity for your data.

### How We Choose the Best Degree

We compare **test MSE** (not training MSE) because it measures generalization.

**Example Results:**

| Degree | Train MSE | Test MSE | RÂ² Score | Result |
|--------|-----------|----------|----------|--------|
| 1 | 326.64 | 455.62 | 0.712 | âŒ Underfit |
| 2 | 137.29 | 143.74 | 0.909 | ğŸ‘ Good |
| 3 | 81.60 | 60.22 | 0.962 | â­ **BEST** |
| 10 | 74.93 | 81.73 | 0.948 | âš ï¸ Overfit |

**Final Model = Degree 3**

It generalizes best and closely matches the true cubic pattern. Notice that degree 10 has lower training MSE but higher test MSE - a classic sign of overfitting.

## ğŸ“ˆ Evaluation Metrics & Results

### Key Metrics Explained

- **Train MSE**: Mean Squared Error on training data (how well model fits training set)
- **Test MSE**: Mean Squared Error on test data (how well model generalizes)
- **RÂ² Score**: Coefficient of determination (proportion of variance explained, 0 to 1)

### Interpreting Results

- **Low Train MSE, High Test MSE** â†’ Overfitting (model memorized training data)
- **High Train MSE, High Test MSE** â†’ Underfitting (model too simple)
- **Low Train MSE, Low Test MSE** â†’ Good fit (model generalizes well)

## ğŸ”¥ Real-World Example: Hours Studied â†’ Polynomial Features

### Scenario Setup

Let's say:
```
X = hours studied
```

If degree = 3:
```
X_poly = [hours, hoursÂ², hoursÂ³]
```

### Feature Interpretation

**Why these features?**

- **hours**: Basic linear increase
- **hoursÂ²**: Captures plateau or diminishing returns
- **hoursÂ³**: Models early struggle + later improvement

This models **REAL human learning** more accurately than a simple straight line.

### Detailed Example: Student Study Hours

**Suppose a student studied 3 hours:**

```
X = [3]
```

If we use degree = 3, `PolynomialFeatures` transforms it into:

```
X_poly = [3, 3Â², 3Â³]
X_poly = [3, 9, 27]
```

### Feature Meaning (Real Interpretation)

#### 1ï¸âƒ£ Feature 1 â€” x = 3

"More hours â†’ more marks" (basic linear growth)

But real learning is **not perfectly linear**.

#### 2ï¸âƒ£ Feature 2 â€” xÂ² = 9

Represents **diminishing returns**:

- From 0 â†’ 3 hours: improvement is sharp
- From 3 â†’ 6 hours: improvement is slower
- Eventually plateaus

So the square gives the curve shape.

**If hours = 5 â†’** `xÂ² = 25` (stronger curvature, bigger bending effect)

#### 3ï¸âƒ£ Feature 3 â€” xÂ³ = 27

Represents **early struggle + late acceleration**:

- In beginning: learning is slow
- After understanding basics: learning accelerates

So cubic terms model more **S-shaped patterns**.

**If hours = 5 â†’** `xÂ³ = 125` (strong effect on S-curve)

### Comparison: Two Students

| Student | Hours Studied (x) | xÂ² | xÂ³ |
|---------|-------------------|----|----|
| A | 3 | 9 | 27 |
| B | 5 | 25 | 125 |

**Interpretation:**

- Student B studied more (5 hours)
- Their `xÂ² = 25` â†’ indicates stronger bend (approaching plateau or diminishing returns)
- Their `xÂ³ = 125` â†’ indicates acceleration (late learning improvement)

The polynomial model uses these three signals to predict marks.

### Why Not Just Use Hours (x)? Why xÂ² and xÂ³?

Because **human learning is NON-LINEAR**:

- **Day 1â€“3**: Big jumps! Concepts are new â†’ steep learning
- **Day 4â€“6**: Plateau â†’ diminishing returns
- **Later**: Understanding clicks â†’ big improvement again

A straight line cannot model this. **Polynomial regression can.**

### Prediction Example

**Assume the trained model learned:**

```
y = 4*x - 1.5*xÂ² + 0.3*xÂ³ + 50
```

**Now apply it:**

**For x = 3 hours:**
```
y = 4(3) - 1.5(9) + 0.3(27) + 50
  = 12 - 13.5 + 8.1 + 50
  = 56.6 marks
```

**For x = 5 hours:**
```
y = 4(5) - 1.5(25) + 0.3(125) + 50
  = 20 - 37.5 + 37.5 + 50
  = 70 marks
```

**See the difference?**

From 3 â†’ 5 hours: only +2 hours, but marks jump from 56.6 â†’ 70.

Because cubic model captures **late surge in learning** - the non-linear acceleration that linear models miss.

### Summary Table

| Hours | x | xÂ² | xÂ³ | Real Meaning |
|-------|---|----|----|--------------|
| 3 | 3 | 9 | 27 | Moderate study |
| 5 | 5 | 25 | 125 | Stronger curve + late improvement |

## ğŸŒ Real-World Applications

Polynomial regression is used in:

- âœ” **Student learning curves** - Modeling academic performance over time
- âœ” **Sales growth patterns** - Forecasting business growth
- âœ” **Medicine dose-response** - Understanding drug effectiveness
- âœ” **Manufacturing cost curves** - Production cost optimization
- âœ” **Priceâ€“demand curves** - Economics and pricing strategies
- âœ” **Biological growth** - Height vs. age, population growth
- âœ” **Marketing ROI curves** - Return on investment analysis
- âœ” **Stock smoothing** - Short-term trend analysis

**All these relationships are non-linear** and benefit from polynomial feature expansion.

## ğŸ–¼ï¸ Visualizations

The `plots.py` script generates curves for all degrees, helping visualize:

- **Degree 1** â†’ Underfitting (straight line misses curvature)
- **Degree 3** â†’ Natural, smooth shape (best fit)
- **Degree 10** â†’ Overfitting (wiggly curve captures noise)

![Polynomial Regression Plots](./plots.png)

This visualization clearly shows:
- How degree 1 underfits the data
- How degree 3 captures the true pattern
- How degree 10 overfits with excessive wiggles

## ğŸ Final Takeaways

You now understand:

- âœ” **Why polynomial regression is powerful** - Captures non-linear relationships
- âœ” **How linear models learn curves** - Using xÂ², xÂ³ features
- âœ” **How underfitting & overfitting appear** - Both in metrics and plots
- âœ” **Why degree 3 was the best choice** - Optimal balance of complexity
- âœ” **How sklearn solves regression instantly** - Once features are expanded
- âœ” **How polynomial regression relates to:**
  - Neural networks (they also create non-linear transforms)
  - Kernel methods
  - Feature engineering
  - Biasâ€“variance tradeoff

## ğŸŒŸ Next Steps

### Project 4 â€” Ridge & Lasso Regression

Regularization solves:

- **Overfitting** (especially degree 10 models)
- **Large coefficients** (unstable predictions)
- **Wiggly curves** (poor generalization)

**You will learn:**

- **Ridge Regression (L2)** - Penalizes large coefficients
- **Lasso Regression (L1)** - Can zero out coefficients (feature selection)
- **How regularization smooths the curve** - Reduces overfitting
- **How to tune regularization strength (alpha)** - Finding optimal balance

Regularization techniques help control model complexity and prevent overfitting, especially useful for high-degree polynomial models.

---

**Note**: This project is part of a Machine Learning Specialization series designed to build foundational understanding through hands-on implementation. It builds on Projects 1 and 2, extending linear models to non-linear relationships while maintaining training efficiency.

ğŸ“˜ Project 13 â€” Random Forest Classifier

Random Forest is one of the most powerful and widely used classical machine learning algorithms for tabular data.

It improves upon Decision Trees by using many trees together instead of relying on just one.

ğŸŒ² What is Random Forest? (Simple Explanation)

A Decision Tree is like one person making a decision

A Random Forest is like asking 100 people and taking a vote

ğŸ‘‰ One tree can be wrong
ğŸ‘‰ Many trees together are much more reliable

Random Forest = Many Decision Trees + Majority Voting

â“ Why Random Forest is Needed
Problem with Decision Trees

Decision Trees:

âŒ Overfit easily

âŒ Memorize training data

âŒ Very sensitive to small data changes

How Random Forest Fixes This

Random Forest:

âœ” Trains many trees

âœ” Each tree sees different data (bootstrapping)

âœ” Each tree sees different features

âœ” Final prediction = majority vote

This leads to:

âœ… Higher accuracy

âœ… Better generalization

âœ… Much less overfitting

ğŸ§  How Random Forest Works (High Level)

Randomly sample data (with replacement)

Train a decision tree on each sample

Limit tree depth to avoid memorization

Each tree makes a prediction

Final prediction = majority vote

This is called Bagging (Bootstrap Aggregation).

ğŸ§ª Dataset Used (Synthetic Data)

We use synthetic data generated with make_classification.

Why synthetic?

Controlled environment

Known class boundaries

Perfect for learning & visualization

Dataset properties:

300 data points

2 features (easy to plot)

2 classes (binary classification)

Train/Test split = 80/20

This simulates real-world structured data like:

customer profiles

credit risk data

medical measurements

ğŸ“‚ Project Structure
project13_random_forest/
â”‚â”€â”€ data.py          # generate synthetic dataset
â”‚â”€â”€ train.py         # train & save model
â”‚â”€â”€ eval.py          # evaluate performance
â”‚â”€â”€ plot.py          # decision boundary visualization
â”‚â”€â”€ RF.png           # saved plot
â”‚â”€â”€ README.md

âš™ï¸ Model Configuration

We train the model using:

RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42
)

Meaning:

n_estimators=100 â†’ 100 decision trees

max_depth=5 â†’ prevents overfitting

random_state=42 â†’ reproducible results

ğŸ“Š Results (Your Output)
Training
Random Forest trained and saved.

Evaluation
Accuracy: 0.95


Classification Report:

Class	Precision	Recall	F1-score
0	0.94	0.97	0.95
1	0.97	0.93	0.95

âœ” High accuracy
âœ” Balanced precision & recall
âœ” Much better than a single decision tree

ğŸ“ˆ Visualization

The decision boundary is saved as:

RF.png


Add this to README:

![Random Forest Decision Boundary](./RF.png)


What the plot shows:

Smooth boundaries

Less noise than Decision Tree

Strong generalization

ğŸ” Random Forest vs Decision Tree vs Logistic Regression
Feature	Logistic Regression	Decision Tree	Random Forest
Model Type	Linear	Rule-based	Ensemble
Handles Non-linearity	âŒ No	âœ… Yes	âœ…âœ… Yes
Overfitting	Low	High	Very Low
Accuracy	Medium	Medium	High
Interpretability	Medium	High	Medium
Industry Usage	High	Medium	Very High
ğŸŒ Real-World Uses of Random Forest

Random Forest is heavily used in industry for:

ğŸ¦ Credit risk scoring

ğŸ’³ Fraud detection

ğŸ¥ Medical diagnosis

ğŸ“‰ Customer churn prediction

ğŸ“Š Tabular business data

ğŸ† Kaggle competitions (baseline model)

Andrew Ngâ€™s advice:
â€œIf you donâ€™t know what model to try first on tabular data â€” use Random Forest.â€

ğŸ§  Why Andrew Ng Teaches This After Decision Trees

Andrew Ngâ€™s teaching order:

Linear models

Logistic regression

Decision trees

Random Forest

Boosting (XGBoost)

Neural Networks

Because Random Forest:

Builds ensemble intuition

Fixes tree weaknesses

Bridges classical ML â†’ advanced ML

Is extremely practical

âœ… What You Mastered in Project 13

âœ” Ensemble learning
âœ” Bagging (bootstrap aggregation)
âœ” Reducing overfitting
âœ” Stability vs variance
âœ” Industry-grade ML modeling

ğŸ”œ Whatâ€™s Next?
â–¶ Project 14 â€” XGBoost / Gradient Boosting

More powerful than Random Forest

Sequential learning

Industry & Kaggle standard

Strong regularization

After that â†’ Neural Networks ğŸš€

If you want, next I can:

âœ… Start Project 14 â€” XGBoost
# Project 6 ‚Äî Logistic Regression using scikit-learn

This project implements binary classification using logistic regression from `sklearn` and evaluates the model using **Accuracy, Precision, Recall, and F1 Score**. It follows after:

- **Project 5** ‚Üí Logistic Regression from scratch  
- **Project 6** ‚Üí Logistic Regression using sklearn

---

## üìã Table of Contents

- [Overview](#overview)
- [What Logistic Regression Does](#what-logistic-regression-does)
- [How Logistic Regression Works (Simple)](#how-logistic-regression-works-simple)
- [Why Logistic Regression ‚â† Linear Regression](#why-logistic-regression--linear-regression)
- [Dataset Used (Synthetic)](#dataset-used-synthetic)
- [Model Training (sklearn)](#model-training-sklearn)
- [Evaluation Metrics](#evaluation-metrics)
- [Your Evaluation Results (Validated)](#your-evaluation-results-validated)
- [Decision Boundary Plot](#decision-boundary-plot)
- [Full Summary](#full-summary)
- [Real-World Example](#real-world-example)
- [Next Project](#next-project)

---

## Overview

Binary classification with scikit-learn‚Äôs `LogisticRegression`, focusing on probability-based decisions and full evaluation (Accuracy, Precision, Recall, F1).

---

## üß† 1. What Logistic Regression Does

Logistic Regression predicts probabilities for two classes:

- Class 0 (negative)  
- Class 1 (positive)

It is used when output is categorical:

| Example           | Output            |
|-------------------|-------------------|
| Spam Detection    | spam / not spam   |
| Tumor Diagnosis   | malignant / benign|
| Credit Default    | yes / no          |
| Exam pass         | pass / fail       |

---

## ‚öôÔ∏è 2. How Logistic Regression Works (Simple)

It starts like Linear Regression:

```
z = Œ∏¬∑x + b
```

But instead of predicting a number, it feeds `z` into the sigmoid function:

**Sigmoid (key concept)**

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

Produces a value between 0 and 1 ‚Üí interprets as probability of class 1.

**Decision rule:**

```
œÉ(z) ‚â• 0.5 ‚áí predict 1
œÉ(z) < 0.5 ‚áí predict 0
```

---

## üß† 3. Why Logistic Regression ‚â† Linear Regression

| Linear Regression | Logistic Regression |
|-------------------|---------------------|
| Predicts continuous values (‚àí‚àû to +‚àû) | Predicts probabilities (0‚Äì1) |
| Uses MSE loss | Uses Binary Cross-Entropy loss |
| Fits a straight line | Fits a sigmoid S-curve |
| Not suitable for classification | Made specifically for classification |

---

## üß™ 4. Dataset Used (Synthetic)

We create a simple classification dataset:

```python
X = np.linspace(0, 10, 200).reshape(-1, 1)
y = (X[:, 0] > 5).astype(int)
```

Meaning:

- If feature value > 5 ‚Üí class 1  
- Else class 0

This gives a perfectly separable dataset, so the model should reach nearly 100% performance.

---

## üìà 5. Model Training (sklearn)

```python
model = LogisticRegression()
model.fit(X, y)
```

After training, you got:

- **Coefficient:** `[[3.19781609]]`  
- **Bias:** `[-15.98901022]`

Meaning:

- Positive coefficient ‚Üí probability increases as x increases  
- Negative bias ‚Üí shifts decision boundary

Decision boundary occurs when:

```
Œ∏x + b = 0  ‚áí  x = ‚àíb/Œ∏
x = ‚àí(‚àí15.989) / 3.197 ‚âà 5.0
```

Perfect ‚Äî model learned exactly what we expect.

---

## üéØ 6. Evaluation Metrics

main questions:

‚ÄúWhat are Precision, Recall, F1 Score and why weren‚Äôt they used in scratch implementation?‚Äù

‚úî **Accuracy**

```
Accuracy = Correct predictions / Total samples
```

Useful when dataset is balanced.

‚úî **Precision**

Of all predicted positives, how many were correct?

```
Precision = TP / (TP + FP)
```

‚úî **Recall**

Of all actual positives, how many did we detect?

```
Recall = TP / (TP + FN)
```

‚úî **F1 Score**

Balanced mean of Precision and Recall.

```
F1 = 2 * Precision * Recall / (Precision + Recall)
```

**Why these were NOT in Project 5 (scratch)?**

Because:

- Scratch projects focus on core math  
- Accuracy was enough for understanding gradient descent  
- Computing F1 manually would distract from logistic regression theory  
- Sklearn makes these metrics easy ‚Üí so we use them here.

---

## üìä 7. Your Evaluation Results (VALIDATED)

- Accuracy: **1.0**  
- Precision: **1.0**  
- Recall: **1.0**  
- F1 Score: **1.0**

These are perfect scores. `1.0` means 100%.

Python prints:

- `1.0` ‚Üí float  
- `100%` ‚Üí percentage format

Both mean the same thing. Because dataset is perfectly separable, sklearn found the exact decision boundary.

---

## üñºÔ∏è 8. Decision Boundary Plot

Save this image as:

```
plot.png
```

Interpretation:

- Red = Class 0  
- Blue = Class 1  
- Green dashed line = decision boundary (x ‚âà 5.0)

Perfect separation ‚Üí perfect metrics.

---

## üèÅ 9. Full Summary

You now understand:

- ‚úî Why logistic regression is used ‚Üí For binary classification using probability.  
- ‚úî What sigmoid does ‚Üí Maps any value to 0‚Äì1 probability.  
- ‚úî How model decides class ‚Üí Threshold at probability = 0.5.  
- ‚úî Why your accuracy = 1.0 ‚Üí Dataset is perfectly separable.  
- ‚úî Why we calculate precision, recall, F1 ‚Üí Deeper insight for real-world (imperfect) datasets.  
- ‚úî Why scratch project didn‚Äôt include them ‚Üí Focused on core algorithm, not evaluation engineering.

---

## üåç 10. Real-World Example

**Email Spam Classification**

| Email Feature: number of suspicious words | Prediction |
|-------------------------------------------|------------|
| value > threshold ‚Üí spam | 1 |
| value < threshold ‚Üí not spam | 0 |

Logistic regression:

- Learns which features increase spam probability  
- Œ∏ = weights ‚Üí importance of each word  
- b = bias ‚Üí base likelihood  
- sigmoid ‚Üí outputs spam probability

Same process as your synthetic x>5 example.

---

## üöÄ 11. Next Project (Project 7 ‚Äì Softmax Regression)

Now that binary classification is complete, next is multiclass classification:

- Predict digit 0‚Äì9  
- Predict sentiment (positive / neutral / negative)  
- Predict iris flower species  

**Softmax is the foundation for neural networks.**
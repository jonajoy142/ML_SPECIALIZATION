# ðŸ“˜ Project 5 â€” Logistic Regression (From Scratch)

This project implements **Logistic Regression manually using NumPy**, without sklearn. It teaches how binary classification works **under the hood** at the most fundamental level.

---

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [What You Will Learn](#what-you-will-learn)
- [Why Logistic Regression Exists](#why-logistic-regression-exists)
- [Sigmoid Function](#sigmoid-function)
- [Model Formula](#model-formula)
- [Q&A (Your Questions Answered)](#qa-your-questions-answered)
- [Example of Prediction](#example-of-prediction)
- [Decision Boundary Plot](#decision-boundary-plot)
- [Your Actual Results](#your-actual-results)
- [Real World Example â€” Spam Classification](#real-world-example--spam-classification)
- [Folder Structure](#folder-structure)
- [How to Run](#how-to-run)
- [Next Project](#next-project)

---

## Overview

Goal: understand and implement Logistic Regression from scratch using NumPy for binary classification, including the math, training loop, and evaluation.

---

## ðŸŽ¯ What You Will Learn

### âœ” 1. The difference between **Linear Regression** and **Logistic Regression**

| Linear Regression       | Logistic Regression        |
| ----------------------- | -------------------------- |
| Predicts numbers        | Predicts classes (0/1)     |
| Output: any real number | Output: probability 0â€“1    |
| Uses straight line      | Uses S-curve (sigmoid)     |
| Loss: MSE               | Loss: Binary Cross Entropy |

---

### âœ” 2. Why Logistic Regression Exists

Because Linear Regression cannot restrict outputs to **0 or 1**.

Classification requires probability â†’ **Logistic regression uses sigmoid**. Sigmoid makes the output look like:

* near **0** â†’ class 0  
* near **1** â†’ class 1

---

## ðŸ” Sigmoid Function

The sigmoid function:

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

It takes any number (âˆ’âˆž to +âˆž) and compresses it to **0â€“1**.

**Intuition:**

* Large negative number â†’ sigmoid â‰ˆ 0  
* Large positive number â†’ sigmoid â‰ˆ 1  
* Zero â†’ sigmoid = 0.5

This is why logistic regression can classify.

---

## ðŸ§® Model Formula

Logistic Regression first computes a **linear part** just like linear regression:

[
z = \theta x + b
]

Then applies sigmoid:

[
\hat{y} = \sigma(z)
]

Finally converts probability â†’ class:

```
if probability >= 0.5 â†’ predict class 1
else â†’ predict class 0
```

---

## ðŸ§  Q&A (Your Questions Answered)

### Q1: Isnâ€™t logistic regression same as linear regression?

âœ” Same **linear core** (`Î¸x + b`)  
âŒ Different **activation**

Linear â†’ outputs number  
Logistic â†’ number â†’ sigmoid â†’ probability

---

### Q2: What does this mean?

`(X[:, 0] > 5).astype(int)`

This creates **labels y** for synthetic data:

* If feature x > 5 â†’ label = 1  
* Else â†’ label = 0

Like real world:

* Transaction amount > â‚¹5000 â†’ fraud (1)  
* Else â†’ not fraud (0)

---

### Q3: Explain this part of training:

```python
for _ in range(self.iterations):
    linear = np.dot(X, self.theta) + self.bias
    y_pred = sigmoid(linear)

    d_theta = (1/m) * np.dot(X.T, (y_pred - y))
    d_bias = (1/m) * np.sum(y_pred - y)

    self.theta -= self.lr * d_theta
    self.bias -= self.lr * d_bias
```

ðŸ”¹ Compute z = Î¸x + b  
ðŸ”¹ Convert z â†’ probability using sigmoid  
ðŸ”¹ Compute gradient of loss  
ðŸ”¹ Update parameters  
ðŸ”¹ Repeat (gradient descent)

Exactly like Linear Regression, except:

ðŸ‘‰ Instead of raw prediction, we apply **sigmoid**.

---

### Q4: Why accuracy only? Should we check more metrics?

For perfectly separable synthetic data:

* No noise  
* Clean boundary at x = 5

Accuracy = enough.

Later real datasets need:

* Precision  
* Recall  
* F1  
* ROC-AUC

---

### Q5: Your understanding?:

> logistic regression is like linear regression but outputs 0/1 via sigmoid and threshold

âœ… YES â€” this is correct.

Final refinement:

* Logistic regression predicts **probability**, not just a number.  
* Decision boundary happens where probability = 0.5.

---

## ðŸ”¢ Example of Prediction (Simple Numbers)

Take:

Î¸ = 1.49  
b = â€“7.26

Predict for x = 3:

```
z = 1.49*3 - 7.26 = -2.79
sigmoid(-2.79) = 0.057 â†’ predicts class 0
```

Predict for x = 9:

```
z = 1.49*9 - 7.26 = 6.14
sigmoid(6.14) = 0.997 â†’ predicts class 1
```

âœ” This is why your model works.

---

## ðŸ“Š Decision Boundary Plot

Add this file to your folder as **plot5.png**:

```
2_Classification/project5_logistic_regression_scratch/plot5.png
```

Embed in README:

![Decision Boundary](./plot5.png)

Interpretation:

* Red points = class 0  
* Blue points = class 1  
* Green dashed line â†’ decision boundary  
* Everything left of line â†’ predicted 0  
* Everything right â†’ predicted 1

---

## ðŸ§ª Your Actual Results (From eval.py)

```
Theta: [1.49655377]
Bias: -7.269057951870945
Accuracy: 0.99
```

Perfect results because:

* Data is clean  
* Classification line at x â‰ƒ 4.59  
* Sigmoid outputs sharp probability changes

---

## ðŸš€ Real World Example â€” Spam Classification

Feature: number of suspicious words  
Label: spam (1) or not spam (0)

[
z = \theta x + b
]

If z very positive â†’ sigmoid â‰ˆ 1 â†’ spam  
If z very negative â†’ sigmoid â‰ˆ 0 â†’ not spam

This is exactly how Gmail spam filters started originally.

---

## ðŸ“¦ Folder Structure

```
project5_logistic_regression_scratch/
â”‚â”€â”€ data.py
â”‚â”€â”€ model.py
â”‚â”€â”€ train.py
â”‚â”€â”€ eval.py
â”‚â”€â”€ plots.py
â”‚â”€â”€ plot5.png
â”‚â”€â”€ README.md
```

---

## ðŸš€ How to Run

### Train:

```
python train.py
```

### Evaluate:

```
python eval.py
```

### Plot decision boundary:

```
python plots.py
```

---

## ðŸŸ¢ Next Project

### **Project 6 â€” Logistic Regression with sklearn**

We will learn:

* `sklearn.linear_model.LogisticRegression`  
* Predict probabilities  
* Plot ROC Curve  
* Multiclass = Softmax Regression

